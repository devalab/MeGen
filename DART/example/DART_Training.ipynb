{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "# import pdb\n",
    "from torch.optim.lr_scheduler import MultiStepLR, ReduceLROnPlateau\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from get_mods import DART_Net, sep_ijkl_dataset\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DART_Net(\n",
       "  (fi1): Linear(in_features=3, out_features=128, bias=True)\n",
       "  (fi2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fj1): Linear(in_features=3, out_features=128, bias=True)\n",
       "  (fj2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fk1): Linear(in_features=3, out_features=128, bias=True)\n",
       "  (fk2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fl1): Linear(in_features=3, out_features=128, bias=True)\n",
       "  (fl2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (inter1): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (inter2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (inter3): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (inter4): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (mask): Linear(in_features=128, out_features=128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DART_model = DART_Net(128, 128, 256, 128, 32).to(device)\n",
    "\n",
    "def init_params(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, a=1.0)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "DART_model.apply(init_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_data = sep_ijkl_dataset(\"../../../data/small_dataset.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train:Validation:Test split  and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = .1\n",
    "test_split = .1\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(desc_data)\n",
    "indices = list(range(dataset_size))\n",
    "splitv = int(np.floor(validation_split * dataset_size))\n",
    "splitt = int(np.floor(test_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices, test_indices = indices[splitt+splitv:], indices[:splitv], indices[splitv:splitt+splitv]\n",
    "\n",
    "# Creating data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "trainloader = DataLoader(desc_data, batch_size=32, sampler=train_sampler)\n",
    "validloader = DataLoader(desc_data, batch_size=32, sampler=valid_sampler)\n",
    "testloader = DataLoader(desc_data, batch_size=32, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(DART_model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=25, verbose=True, eps=1e-09)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochal_train_losses = []\n",
    "epochal_val_losses  = []\n",
    "num_epochs = 1500\n",
    "epoch_freq = 1\n",
    "       \n",
    "def test(DART_model, testloader):\n",
    "    mae = torch.nn.L1Loss()\n",
    "    rmse = torch.nn.MSELoss()\n",
    "    pred_energy = torch.tensor([], device=\"cuda\")\n",
    "    real_energy = torch.tensor([], device=\"cuda\")\n",
    "    cluster_size = torch.tensor([], device=\"cuda\")\n",
    "    DART_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            energy = DART_model(batch[\"atm_i\"], batch[\"atm_j\"], batch[\"atm_k\"], batch[\"atm_l\"])\n",
    "            energy = energy.sum(axis=1).squeeze(dim=1)\n",
    "            pred_energy = torch.cat((pred_energy, energy))\n",
    "            real_energy = torch.cat((real_energy, batch[\"energy\"]))\n",
    "            cluster_size = torch.cat((cluster_size, batch[\"atm_i\"][:,0].sum(axis=1)))\n",
    "        results = torch.stack((cluster_size, real_energy, pred_energy), axis=1)\n",
    "        test_loss = mae(pred_energy, real_energy)\n",
    "        rmse_loss = torch.sqrt(rmse(pred_energy, real_energy))\n",
    "        print(\"Test MAE = \", test_loss.item(), \"Test RMSE = \", rmse_loss.item())\n",
    "        return results, test_loss, rmse_loss\n",
    "    \n",
    "def train(DART_model, optimizer, epochal_train_losses, criterion):\n",
    "    train_loss = 0.00\n",
    "    n = 0\n",
    "    DART_model.train()\n",
    "    for batch in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        energy = DART_model(batch[\"atm_i\"], batch[\"atm_j\"], batch[\"atm_k\"], batch[\"atm_l\"])\n",
    "        energy = energy.sum(axis=1)\n",
    "        batch_loss = criterion(energy, batch[\"energy\"].unsqueeze(dim=1))\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += batch_loss.detach().cpu()\n",
    "        n += 1\n",
    "    train_loss /= n\n",
    "    epochal_train_losses.append(train_loss)\n",
    "\n",
    "def train_and_evaluate(DART_model, optimizer, scheduler, criterion, start_epoch=1, restart=None):\n",
    "    if restart:\n",
    "        restore_path = os.path.join(log_dir + \"/last.pth.tar\")\n",
    "        checkpoint = load_checkpoint(restore_path, DART_model, optimizer)\n",
    "        start_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "    best_val = 100000.00\n",
    "    early_stopping_learning_rate = 1.0E-8\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        learning_rate = optimizer.param_groups[0]['lr']\n",
    "        if learning_rate < early_stopping_learning_rate:\n",
    "            break\n",
    "\n",
    "        ############ training #############\n",
    "        train(DART_model, optimizer, epochal_train_losses, criterion)\n",
    "        \n",
    "        ############ validation #############\n",
    "        n=0\n",
    "        val_loss = 0.0\n",
    "        DART_model.eval()\n",
    "        for batch in validloader:\n",
    "            energy = DART_model(batch[\"atm_i\"], batch[\"atm_j\"], batch[\"atm_k\"], batch[\"atm_l\"])\n",
    "            energy = energy.sum(axis=1)\n",
    "            batch_loss = criterion(energy, batch[\"energy\"].unsqueeze(dim=1))\n",
    "            val_loss += batch_loss.detach().cpu()\n",
    "            n += 1\n",
    "        val_loss /= n\n",
    "        epochal_val_losses.append(val_loss)\n",
    "        scheduler.step(val_loss)\n",
    "     \n",
    "        is_best = val_loss <= best_val\n",
    "        if epoch % epoch_freq == 0:\n",
    "            print(\"Epoch: {: <5} Train: {: <20} Val: {: <20}\".format(epoch, epochal_train_losses[-1], val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1     Train: 607.5888061523438    Val: 98.2199935913086    \n",
      "Epoch: 2     Train: 154.45396423339844   Val: 114.55366516113281  \n",
      "Epoch: 3     Train: 113.06208038330078   Val: 105.64413452148438  \n",
      "Epoch: 4     Train: 91.88636779785156    Val: 79.17213439941406   \n",
      "Epoch: 5     Train: 92.08629608154297    Val: 100.1849594116211   \n",
      "Epoch: 6     Train: 80.32140350341797    Val: 70.15669250488281   \n",
      "Epoch: 7     Train: 82.54194641113281    Val: 64.58738708496094   \n",
      "Epoch: 8     Train: 83.78430938720703    Val: 60.843116760253906  \n",
      "Epoch: 9     Train: 72.4614028930664     Val: 72.23690032958984   \n",
      "Epoch: 10    Train: 75.03366088867188    Val: 67.89266967773438   \n",
      "Epoch: 11    Train: 69.69514465332031    Val: 56.97750473022461   \n",
      "Epoch: 12    Train: 75.38196563720703    Val: 53.610084533691406  \n",
      "Epoch: 13    Train: 61.73081970214844    Val: 70.0972900390625    \n",
      "Epoch: 14    Train: 77.32492065429688    Val: 81.3260269165039    \n",
      "Epoch: 15    Train: 61.460914611816406   Val: 55.5780143737793    \n",
      "Epoch: 16    Train: 65.02539825439453    Val: 69.26266479492188   \n",
      "Epoch: 17    Train: 59.598567962646484   Val: 83.59314727783203   \n",
      "Epoch: 18    Train: 54.93491744995117    Val: 51.243125915527344  \n",
      "Epoch: 19    Train: 58.0526123046875     Val: 45.652130126953125  \n",
      "Epoch: 20    Train: 56.348548889160156   Val: 39.105560302734375  \n",
      "Epoch: 21    Train: 51.196067810058594   Val: 37.64582061767578   \n",
      "Epoch: 22    Train: 50.95302200317383    Val: 48.68251037597656   \n",
      "Epoch: 23    Train: 46.59469223022461    Val: 32.616153717041016  \n",
      "Epoch: 24    Train: 42.527645111083984   Val: 33.56867218017578   \n",
      "Epoch: 25    Train: 43.26773452758789    Val: 92.0324478149414    \n",
      "Epoch: 26    Train: 48.651512145996094   Val: 30.307445526123047  \n",
      "Epoch: 27    Train: 34.120304107666016   Val: 53.6386833190918    \n",
      "Epoch: 28    Train: 41.74580383300781    Val: 32.399505615234375  \n",
      "Epoch: 29    Train: 34.475582122802734   Val: 27.152463912963867  \n",
      "Epoch: 30    Train: 38.69602966308594    Val: 63.26600646972656   \n",
      "Epoch: 31    Train: 33.73466491699219    Val: 32.372371673583984  \n",
      "Epoch: 32    Train: 33.385372161865234   Val: 27.006864547729492  \n",
      "Epoch: 33    Train: 35.307464599609375   Val: 41.647361755371094  \n",
      "Epoch: 34    Train: 28.474987030029297   Val: 19.180007934570312  \n",
      "Epoch: 35    Train: 29.811138153076172   Val: 27.45117950439453   \n",
      "Epoch: 36    Train: 32.391746520996094   Val: 19.777236938476562  \n",
      "Epoch: 37    Train: 26.71670150756836    Val: 42.47038650512695   \n",
      "Epoch: 38    Train: 29.705537796020508   Val: 29.88973617553711   \n",
      "Epoch: 39    Train: 29.729646682739258   Val: 74.19683837890625   \n",
      "Epoch: 40    Train: 29.69027328491211    Val: 18.57434844970703   \n",
      "Epoch: 41    Train: 32.61764144897461    Val: 22.419981002807617  \n",
      "Epoch: 42    Train: 26.111135482788086   Val: 20.712787628173828  \n",
      "Epoch: 43    Train: 36.01648712158203    Val: 38.1302490234375    \n",
      "Epoch: 44    Train: 24.414657592773438   Val: 23.472816467285156  \n",
      "Epoch: 45    Train: 25.811307907104492   Val: 47.83671569824219   \n",
      "Epoch: 46    Train: 29.723609924316406   Val: 20.86648941040039   \n",
      "Epoch: 47    Train: 24.721895217895508   Val: 31.1238956451416    \n",
      "Epoch: 48    Train: 23.663005828857422   Val: 22.874616622924805  \n",
      "Epoch: 49    Train: 25.9460506439209     Val: 19.245433807373047  \n",
      "Epoch: 50    Train: 25.129724502563477   Val: 41.20183181762695   \n",
      "Epoch: 51    Train: 31.176681518554688   Val: 18.88831329345703   \n",
      "Epoch: 52    Train: 30.785907745361328   Val: 27.623422622680664  \n",
      "Epoch: 53    Train: 29.276447296142578   Val: 24.347824096679688  \n",
      "Epoch: 54    Train: 27.217321395874023   Val: 74.79496002197266   \n",
      "Epoch: 55    Train: 24.746173858642578   Val: 17.786563873291016  \n",
      "Epoch: 56    Train: 25.180553436279297   Val: 15.981393814086914  \n",
      "Epoch: 57    Train: 24.55719757080078    Val: 12.933929443359375  \n",
      "Epoch: 58    Train: 23.276748657226562   Val: 30.807085037231445  \n",
      "Epoch: 59    Train: 23.120996475219727   Val: 15.849954605102539  \n",
      "Epoch: 60    Train: 24.485149383544922   Val: 15.100818634033203  \n",
      "Epoch: 61    Train: 22.32927703857422    Val: 16.08528709411621   \n",
      "Epoch: 62    Train: 21.1615047454834     Val: 18.512426376342773  \n",
      "Epoch: 63    Train: 21.328166961669922   Val: 13.729159355163574  \n",
      "Epoch: 64    Train: 26.26186752319336    Val: 19.539440155029297  \n",
      "Epoch: 65    Train: 22.154069900512695   Val: 34.13645553588867   \n",
      "Epoch: 66    Train: 23.042503356933594   Val: 30.180866241455078  \n",
      "Epoch: 67    Train: 22.80693817138672    Val: 19.50274085998535   \n",
      "Epoch: 68    Train: 26.977731704711914   Val: 15.586236953735352  \n",
      "Epoch: 69    Train: 24.819046020507812   Val: 27.35516357421875   \n",
      "Epoch: 70    Train: 20.261940002441406   Val: 19.62969398498535   \n",
      "Epoch: 71    Train: 25.127336502075195   Val: 18.4810848236084    \n",
      "Epoch: 72    Train: 23.040016174316406   Val: 12.213000297546387  \n",
      "Epoch: 73    Train: 21.712322235107422   Val: 22.798377990722656  \n",
      "Epoch: 74    Train: 21.078659057617188   Val: 12.412104606628418  \n",
      "Epoch: 75    Train: 24.722597122192383   Val: 39.404449462890625  \n",
      "Epoch: 76    Train: 19.5989933013916     Val: 29.3569393157959    \n",
      "Epoch: 77    Train: 21.12921905517578    Val: 18.74016571044922   \n",
      "Epoch: 78    Train: 22.39828109741211    Val: 17.557741165161133  \n",
      "Epoch: 79    Train: 23.359607696533203   Val: 14.511592864990234  \n",
      "Epoch: 80    Train: 20.025575637817383   Val: 21.522340774536133  \n",
      "Epoch: 81    Train: 19.213987350463867   Val: 21.866809844970703  \n",
      "Epoch: 82    Train: 19.250722885131836   Val: 22.094581604003906  \n",
      "Epoch: 83    Train: 22.362937927246094   Val: 52.568233489990234  \n",
      "Epoch: 84    Train: 18.585586547851562   Val: 25.551246643066406  \n",
      "Epoch: 85    Train: 17.8404483795166     Val: 18.139568328857422  \n",
      "Epoch: 86    Train: 20.46526336669922    Val: 46.903018951416016  \n",
      "Epoch: 87    Train: 17.882171630859375   Val: 13.456926345825195  \n",
      "Epoch: 88    Train: 21.52825355529785    Val: 13.878907203674316  \n",
      "Epoch: 89    Train: 19.605680465698242   Val: 13.51296615600586   \n",
      "Epoch: 90    Train: 19.3566837310791     Val: 31.723825454711914  \n",
      "Epoch: 91    Train: 18.86752700805664    Val: 23.107166290283203  \n",
      "Epoch: 92    Train: 20.6824893951416     Val: 11.549355506896973  \n",
      "Epoch: 93    Train: 22.079011917114258   Val: 23.632741928100586  \n",
      "Epoch: 94    Train: 16.478479385375977   Val: 25.175060272216797  \n",
      "Epoch: 95    Train: 19.784788131713867   Val: 20.92007827758789   \n",
      "Epoch: 96    Train: 20.92367172241211    Val: 11.409767150878906  \n",
      "Epoch: 97    Train: 17.552988052368164   Val: 34.048583984375     \n",
      "Epoch: 98    Train: 20.076622009277344   Val: 10.430500030517578  \n",
      "Epoch: 99    Train: 21.499910354614258   Val: 11.653919219970703  \n",
      "Epoch: 100   Train: 16.704132080078125   Val: 9.710598945617676   \n",
      "Epoch: 101   Train: 17.574106216430664   Val: 30.344587326049805  \n",
      "Epoch: 102   Train: 16.939960479736328   Val: 11.805737495422363  \n",
      "Epoch: 103   Train: 16.203828811645508   Val: 8.917420387268066   \n",
      "Epoch: 104   Train: 19.554662704467773   Val: 13.69339370727539   \n",
      "Epoch: 105   Train: 20.929441452026367   Val: 19.736434936523438  \n",
      "Epoch: 106   Train: 18.96678924560547    Val: 16.639501571655273  \n",
      "Epoch: 107   Train: 20.547216415405273   Val: 45.769779205322266  \n",
      "Epoch: 108   Train: 19.238815307617188   Val: 10.010009765625     \n",
      "Epoch: 109   Train: 17.261566162109375   Val: 31.206335067749023  \n",
      "Epoch: 110   Train: 17.258987426757812   Val: 12.939925193786621  \n",
      "Epoch: 111   Train: 19.50690460205078    Val: 13.9516019821167    \n",
      "Epoch: 112   Train: 18.874584197998047   Val: 8.829160690307617   \n",
      "Epoch: 113   Train: 19.0032958984375     Val: 9.107159614562988   \n",
      "Epoch: 114   Train: 18.126789093017578   Val: 9.947189331054688   \n",
      "Epoch: 115   Train: 22.07209014892578    Val: 12.508441925048828  \n",
      "Epoch: 116   Train: 20.744054794311523   Val: 10.359139442443848  \n",
      "Epoch: 117   Train: 17.170059204101562   Val: 26.22478675842285   \n",
      "Epoch: 118   Train: 19.176050186157227   Val: 17.323793411254883  \n",
      "Epoch: 119   Train: 16.19403839111328    Val: 10.916569709777832  \n",
      "Epoch: 120   Train: 18.558208465576172   Val: 26.50763702392578   \n",
      "Epoch: 121   Train: 16.846515655517578   Val: 7.774972438812256   \n",
      "Epoch: 122   Train: 18.19342613220215    Val: 56.93132781982422   \n",
      "Epoch: 123   Train: 16.26434326171875    Val: 13.499433517456055  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124   Train: 18.01376724243164    Val: 9.578516006469727   \n",
      "Epoch: 125   Train: 16.784500122070312   Val: 14.237857818603516  \n",
      "Epoch: 126   Train: 16.548280715942383   Val: 13.644103050231934  \n",
      "Epoch: 127   Train: 15.850751876831055   Val: 9.539752006530762   \n",
      "Epoch: 128   Train: 15.491607666015625   Val: 10.4926118850708    \n",
      "Epoch: 129   Train: 20.16929817199707    Val: 22.875381469726562  \n",
      "Epoch: 130   Train: 18.072776794433594   Val: 9.820626258850098   \n",
      "Epoch: 131   Train: 18.898061752319336   Val: 14.741153717041016  \n",
      "Epoch: 132   Train: 22.14443588256836    Val: 14.820761680603027  \n",
      "Epoch: 133   Train: 21.544261932373047   Val: 22.468612670898438  \n",
      "Epoch: 134   Train: 15.687050819396973   Val: 21.19601058959961   \n",
      "Epoch: 135   Train: 19.04374885559082    Val: 13.310157775878906  \n",
      "Epoch: 136   Train: 15.992230415344238   Val: 8.433335304260254   \n",
      "Epoch: 137   Train: 18.844850540161133   Val: 23.402484893798828  \n",
      "Epoch: 138   Train: 19.87254524230957    Val: 15.8121337890625    \n",
      "Epoch: 139   Train: 20.49041748046875    Val: 13.479639053344727  \n",
      "Epoch: 140   Train: 18.630447387695312   Val: 15.614629745483398  \n",
      "Epoch: 141   Train: 16.211132049560547   Val: 28.123865127563477  \n",
      "Epoch: 142   Train: 15.432236671447754   Val: 7.776618480682373   \n",
      "Epoch: 143   Train: 12.718122482299805   Val: 22.819185256958008  \n",
      "Epoch: 144   Train: 16.972614288330078   Val: 11.845022201538086  \n",
      "Epoch: 145   Train: 17.97576141357422    Val: 6.524316310882568   \n",
      "Epoch: 146   Train: 17.276918411254883   Val: 32.137332916259766  \n",
      "Epoch: 147   Train: 16.97344970703125    Val: 14.900766372680664  \n",
      "Epoch: 148   Train: 14.486983299255371   Val: 9.736589431762695   \n",
      "Epoch: 149   Train: 17.46996307373047    Val: 7.264375686645508   \n",
      "Epoch: 150   Train: 17.760669708251953   Val: 14.81143569946289   \n",
      "Epoch: 151   Train: 16.759889602661133   Val: 7.5053815841674805  \n",
      "Epoch: 152   Train: 17.42495346069336    Val: 13.575654029846191  \n",
      "Epoch: 153   Train: 13.450387954711914   Val: 7.9350457191467285  \n",
      "Epoch: 154   Train: 18.988683700561523   Val: 21.74008560180664   \n",
      "Epoch: 155   Train: 16.068021774291992   Val: 8.816433906555176   \n",
      "Epoch: 156   Train: 24.381446838378906   Val: 14.298686981201172  \n",
      "Epoch: 157   Train: 14.818633079528809   Val: 7.355475425720215   \n",
      "Epoch: 158   Train: 14.083992004394531   Val: 15.542821884155273  \n",
      "Epoch: 159   Train: 22.372282028198242   Val: 10.128921508789062  \n",
      "Epoch: 160   Train: 12.109142303466797   Val: 24.50649642944336   \n",
      "Epoch: 161   Train: 16.66156768798828    Val: 9.450946807861328   \n",
      "Epoch: 162   Train: 17.386043548583984   Val: 9.262170791625977   \n",
      "Epoch: 163   Train: 18.715312957763672   Val: 8.91379451751709    \n",
      "Epoch: 164   Train: 14.414834976196289   Val: 12.190849304199219  \n",
      "Epoch: 165   Train: 17.84832000732422    Val: 15.52296257019043   \n",
      "Epoch: 166   Train: 13.804553985595703   Val: 10.008291244506836  \n",
      "Epoch: 167   Train: 14.02721881866455    Val: 6.2815399169921875  \n",
      "Epoch: 168   Train: 16.584915161132812   Val: 43.45424270629883   \n",
      "Epoch: 169   Train: 17.741891860961914   Val: 22.331497192382812  \n",
      "Epoch: 170   Train: 18.385250091552734   Val: 19.124494552612305  \n",
      "Epoch: 171   Train: 15.324624061584473   Val: 14.643562316894531  \n",
      "Epoch: 172   Train: 15.00019645690918    Val: 6.702540397644043   \n",
      "Epoch: 173   Train: 18.00696563720703    Val: 23.45969009399414   \n",
      "Epoch: 174   Train: 15.019574165344238   Val: 7.485041618347168   \n",
      "Epoch: 175   Train: 15.124419212341309   Val: 15.687016487121582  \n",
      "Epoch: 176   Train: 13.287378311157227   Val: 19.379301071166992  \n",
      "Epoch: 177   Train: 14.862613677978516   Val: 8.721500396728516   \n",
      "Epoch: 178   Train: 17.93827247619629    Val: 13.544881820678711  \n",
      "Epoch: 179   Train: 17.6835994720459     Val: 11.638615608215332  \n",
      "Epoch: 180   Train: 17.053279876708984   Val: 20.469942092895508  \n",
      "Epoch: 181   Train: 17.018930435180664   Val: 22.146648406982422  \n",
      "Epoch: 182   Train: 13.195704460144043   Val: 11.144217491149902  \n",
      "Epoch: 183   Train: 16.259437561035156   Val: 15.179770469665527  \n",
      "Epoch: 184   Train: 17.535106658935547   Val: 9.063529014587402   \n",
      "Epoch: 185   Train: 13.616909980773926   Val: 27.243976593017578  \n",
      "Epoch: 186   Train: 14.267840385437012   Val: 6.252880096435547   \n",
      "Epoch: 187   Train: 13.470039367675781   Val: 7.4219746589660645  \n",
      "Epoch: 188   Train: 13.378650665283203   Val: 24.347200393676758  \n",
      "Epoch: 189   Train: 16.64409637451172    Val: 25.45754623413086   \n",
      "Epoch: 190   Train: 15.158088684082031   Val: 16.69243621826172   \n",
      "Epoch: 191   Train: 14.968269348144531   Val: 9.793536186218262   \n",
      "Epoch: 192   Train: 16.58844566345215    Val: 7.029477596282959   \n",
      "Epoch: 193   Train: 16.965490341186523   Val: 9.455034255981445   \n",
      "Epoch: 194   Train: 13.890408515930176   Val: 8.400969505310059   \n",
      "Epoch: 195   Train: 22.21123695373535    Val: 13.854972839355469  \n",
      "Epoch: 196   Train: 17.182571411132812   Val: 36.396820068359375  \n",
      "Epoch: 197   Train: 17.810504913330078   Val: 15.400995254516602  \n",
      "Epoch: 198   Train: 16.30453109741211    Val: 7.5110039710998535  \n",
      "Epoch: 199   Train: 14.233351707458496   Val: 10.011887550354004  \n",
      "Epoch: 200   Train: 12.703378677368164   Val: 18.02989959716797   \n",
      "Epoch: 201   Train: 14.418391227722168   Val: 12.684856414794922  \n",
      "Epoch: 202   Train: 16.862890243530273   Val: 29.262311935424805  \n",
      "Epoch: 203   Train: 16.89019775390625    Val: 6.504882335662842   \n",
      "Epoch: 204   Train: 16.648845672607422   Val: 6.067409038543701   \n",
      "Epoch: 205   Train: 12.369455337524414   Val: 18.181377410888672  \n",
      "Epoch: 206   Train: 14.589179992675781   Val: 9.192205429077148   \n",
      "Epoch: 207   Train: 16.844371795654297   Val: 9.006162643432617   \n",
      "Epoch: 208   Train: 19.060400009155273   Val: 7.267971038818359   \n",
      "Epoch: 209   Train: 12.776588439941406   Val: 20.53744888305664   \n",
      "Epoch: 210   Train: 19.137605667114258   Val: 7.09063720703125    \n",
      "Epoch: 211   Train: 18.28421401977539    Val: 12.422951698303223  \n",
      "Epoch: 212   Train: 13.0225830078125     Val: 32.355499267578125  \n",
      "Epoch: 213   Train: 16.201740264892578   Val: 24.601396560668945  \n",
      "Epoch: 214   Train: 13.771808624267578   Val: 6.834369659423828   \n",
      "Epoch: 215   Train: 14.819994926452637   Val: 12.340381622314453  \n",
      "Epoch: 216   Train: 13.535627365112305   Val: 8.56445026397705    \n",
      "Epoch: 217   Train: 11.706225395202637   Val: 41.44832992553711   \n",
      "Epoch: 218   Train: 13.544713973999023   Val: 5.720211982727051   \n",
      "Epoch: 219   Train: 16.141395568847656   Val: 24.436250686645508  \n",
      "Epoch: 220   Train: 13.153473854064941   Val: 7.5224714279174805  \n",
      "Epoch: 221   Train: 16.323759078979492   Val: 24.080289840698242  \n",
      "Epoch: 222   Train: 13.179973602294922   Val: 5.9456682205200195  \n",
      "Epoch: 223   Train: 17.982990264892578   Val: 16.66893768310547   \n",
      "Epoch: 224   Train: 14.70747184753418    Val: 12.991399765014648  \n",
      "Epoch: 225   Train: 12.501102447509766   Val: 7.375034809112549   \n",
      "Epoch: 226   Train: 16.29668617248535    Val: 6.7237982749938965  \n",
      "Epoch: 227   Train: 12.538795471191406   Val: 10.000249862670898  \n",
      "Epoch: 228   Train: 16.048107147216797   Val: 10.197464942932129  \n",
      "Epoch: 229   Train: 17.629114151000977   Val: 14.602483749389648  \n",
      "Epoch: 230   Train: 13.345455169677734   Val: 13.361662864685059  \n",
      "Epoch: 231   Train: 15.093623161315918   Val: 11.943730354309082  \n",
      "Epoch: 232   Train: 11.974625587463379   Val: 11.528888702392578  \n",
      "Epoch: 233   Train: 11.006414413452148   Val: 6.603862762451172   \n",
      "Epoch: 234   Train: 15.183069229125977   Val: 16.492198944091797  \n",
      "Epoch: 235   Train: 12.530213356018066   Val: 16.507593154907227  \n",
      "Epoch: 236   Train: 14.46451187133789    Val: 6.085371494293213   \n",
      "Epoch: 237   Train: 13.821799278259277   Val: 18.047405242919922  \n",
      "Epoch: 238   Train: 12.235739707946777   Val: 14.610363006591797  \n",
      "Epoch: 239   Train: 12.851048469543457   Val: 13.01563835144043   \n",
      "Epoch: 240   Train: 15.119603157043457   Val: 11.918362617492676  \n",
      "Epoch: 241   Train: 10.795083045959473   Val: 8.353025436401367   \n",
      "Epoch: 242   Train: 14.377928733825684   Val: 11.883454322814941  \n",
      "Epoch: 243   Train: 14.584933280944824   Val: 15.600703239440918  \n",
      "Epoch   244: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 244   Train: 13.970514297485352   Val: 9.953641891479492   \n",
      "Epoch: 245   Train: 5.647101402282715    Val: 4.908735275268555   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 246   Train: 5.031068801879883    Val: 5.464229106903076   \n",
      "Epoch: 247   Train: 4.920995712280273    Val: 4.835978031158447   \n",
      "Epoch: 248   Train: 4.760519981384277    Val: 5.230434417724609   \n",
      "Epoch: 249   Train: 4.752709865570068    Val: 4.523773670196533   \n",
      "Epoch: 250   Train: 4.722417831420898    Val: 4.720516204833984   \n",
      "Epoch: 251   Train: 4.744997501373291    Val: 5.872957706451416   \n",
      "Epoch: 252   Train: 4.9619526863098145   Val: 6.001497268676758   \n",
      "Epoch: 253   Train: 4.732927322387695    Val: 6.591241359710693   \n",
      "Epoch: 254   Train: 4.864285945892334    Val: 5.198471546173096   \n",
      "Epoch: 255   Train: 4.556281566619873    Val: 4.70335578918457    \n",
      "Epoch: 256   Train: 4.739108562469482    Val: 4.367654800415039   \n",
      "Epoch: 257   Train: 4.422621726989746    Val: 5.35255241394043    \n",
      "Epoch: 258   Train: 4.572192192077637    Val: 5.5058135986328125  \n",
      "Epoch: 259   Train: 4.595459461212158    Val: 4.316839218139648   \n",
      "Epoch: 260   Train: 4.5155816078186035   Val: 8.362969398498535   \n",
      "Epoch: 261   Train: 4.631468296051025    Val: 5.832256317138672   \n",
      "Epoch: 262   Train: 4.5550618171691895   Val: 6.191555976867676   \n",
      "Epoch: 263   Train: 4.695569038391113    Val: 4.210238933563232   \n",
      "Epoch: 264   Train: 4.474799633026123    Val: 4.504464149475098   \n",
      "Epoch: 265   Train: 4.758373260498047    Val: 4.691198825836182   \n",
      "Epoch: 266   Train: 4.389286994934082    Val: 4.403517723083496   \n",
      "Epoch: 267   Train: 4.226794242858887    Val: 4.698714733123779   \n",
      "Epoch: 268   Train: 4.522537708282471    Val: 4.507447719573975   \n",
      "Epoch: 269   Train: 4.8783860206604      Val: 5.747654914855957   \n",
      "Epoch: 270   Train: 4.526074409484863    Val: 4.988828659057617   \n",
      "Epoch: 271   Train: 4.391605377197266    Val: 4.611820220947266   \n",
      "Epoch: 272   Train: 4.5278239250183105   Val: 4.187689781188965   \n",
      "Epoch: 273   Train: 4.48549222946167     Val: 4.2931437492370605  \n",
      "Epoch: 274   Train: 4.396332740783691    Val: 4.294623851776123   \n",
      "Epoch: 275   Train: 4.399492263793945    Val: 4.237652778625488   \n",
      "Epoch: 276   Train: 4.614757537841797    Val: 7.549709320068359   \n",
      "Epoch: 277   Train: 4.3400044441223145   Val: 4.120896816253662   \n",
      "Epoch: 278   Train: 4.4816575050354      Val: 4.858404636383057   \n",
      "Epoch: 279   Train: 4.2753400802612305   Val: 4.070670127868652   \n",
      "Epoch: 280   Train: 4.67220401763916     Val: 4.040737628936768   \n",
      "Epoch: 281   Train: 4.396756172180176    Val: 4.253650665283203   \n",
      "Epoch: 282   Train: 4.168746471405029    Val: 4.371142387390137   \n",
      "Epoch: 283   Train: 4.416074275970459    Val: 4.106072425842285   \n",
      "Epoch: 284   Train: 4.245528697967529    Val: 4.875604152679443   \n",
      "Epoch: 285   Train: 4.173588752746582    Val: 5.4557108879089355  \n",
      "Epoch: 286   Train: 4.258574485778809    Val: 4.408332347869873   \n",
      "Epoch: 287   Train: 4.344489574432373    Val: 7.624694347381592   \n",
      "Epoch: 288   Train: 4.728478908538818    Val: 4.527000904083252   \n",
      "Epoch: 289   Train: 4.331210613250732    Val: 4.77944278717041    \n",
      "Epoch: 290   Train: 4.207395076751709    Val: 4.6113057136535645  \n",
      "Epoch: 291   Train: 4.268702507019043    Val: 3.9795899391174316  \n",
      "Epoch: 292   Train: 4.126858234405518    Val: 5.808814525604248   \n",
      "Epoch: 293   Train: 4.548313617706299    Val: 5.352569580078125   \n",
      "Epoch: 294   Train: 4.111090183258057    Val: 4.19157600402832    \n",
      "Epoch: 295   Train: 4.249403953552246    Val: 4.066550254821777   \n",
      "Epoch: 296   Train: 4.357935428619385    Val: 4.2477006912231445  \n",
      "Epoch: 297   Train: 4.322035312652588    Val: 4.87391471862793    \n",
      "Epoch: 298   Train: 4.083082675933838    Val: 3.9782145023345947  \n",
      "Epoch: 299   Train: 4.068796634674072    Val: 4.063002586364746   \n",
      "Epoch: 300   Train: 4.093612194061279    Val: 4.843064785003662   \n",
      "Epoch: 301   Train: 4.64132022857666     Val: 3.9990789890289307  \n",
      "Epoch: 302   Train: 4.180985450744629    Val: 4.082630157470703   \n",
      "Epoch: 303   Train: 4.6873602867126465   Val: 4.862002372741699   \n",
      "Epoch: 304   Train: 4.35267448425293     Val: 3.892914056777954   \n",
      "Epoch: 305   Train: 4.443398952484131    Val: 4.09042501449585    \n",
      "Epoch: 306   Train: 4.564795970916748    Val: 7.1161208152771     \n",
      "Epoch: 307   Train: 4.187445640563965    Val: 5.414036750793457   \n",
      "Epoch: 308   Train: 4.298496723175049    Val: 4.517144203186035   \n",
      "Epoch: 309   Train: 4.077040195465088    Val: 4.330442428588867   \n",
      "Epoch: 310   Train: 4.124616622924805    Val: 5.995067119598389   \n",
      "Epoch: 311   Train: 4.169568061828613    Val: 5.69041633605957    \n",
      "Epoch: 312   Train: 4.451296806335449    Val: 4.024200916290283   \n",
      "Epoch: 313   Train: 4.230039596557617    Val: 4.7960710525512695  \n",
      "Epoch: 314   Train: 4.152071952819824    Val: 3.996757745742798   \n",
      "Epoch: 315   Train: 4.230638027191162    Val: 4.392326354980469   \n",
      "Epoch: 316   Train: 4.129939556121826    Val: 3.8824617862701416  \n",
      "Epoch: 317   Train: 4.224492073059082    Val: 3.9184703826904297  \n",
      "Epoch: 318   Train: 4.148367881774902    Val: 3.9709839820861816  \n",
      "Epoch: 319   Train: 4.289595603942871    Val: 4.212469577789307   \n",
      "Epoch: 320   Train: 4.139883995056152    Val: 4.110734939575195   \n",
      "Epoch: 321   Train: 4.05311393737793     Val: 4.420111179351807   \n",
      "Epoch: 322   Train: 4.276958465576172    Val: 4.087584972381592   \n",
      "Epoch: 323   Train: 4.144314765930176    Val: 3.868422031402588   \n",
      "Epoch: 324   Train: 4.033637046813965    Val: 4.971977710723877   \n",
      "Epoch: 325   Train: 4.2794389724731445   Val: 4.395711898803711   \n",
      "Epoch: 326   Train: 4.399651527404785    Val: 3.861082077026367   \n",
      "Epoch: 327   Train: 4.165624618530273    Val: 5.246799468994141   \n",
      "Epoch: 328   Train: 4.28767204284668     Val: 4.138680934906006   \n",
      "Epoch: 329   Train: 4.004902362823486    Val: 5.011031627655029   \n",
      "Epoch: 330   Train: 4.125749588012695    Val: 4.060266971588135   \n",
      "Epoch: 331   Train: 3.8829052448272705   Val: 5.096410751342773   \n",
      "Epoch: 332   Train: 4.34091329574585     Val: 5.249873161315918   \n",
      "Epoch: 333   Train: 4.240823745727539    Val: 3.8095169067382812  \n",
      "Epoch: 334   Train: 4.218043327331543    Val: 4.0863938331604     \n",
      "Epoch: 335   Train: 4.309138298034668    Val: 4.400140285491943   \n",
      "Epoch: 336   Train: 3.970691442489624    Val: 4.225935935974121   \n",
      "Epoch: 337   Train: 4.298274040222168    Val: 3.9998843669891357  \n",
      "Epoch: 338   Train: 4.047043323516846    Val: 4.293206214904785   \n",
      "Epoch: 339   Train: 4.236598014831543    Val: 5.257138729095459   \n",
      "Epoch: 340   Train: 4.060571670532227    Val: 4.434854984283447   \n",
      "Epoch: 341   Train: 4.150728225708008    Val: 3.8307993412017822  \n",
      "Epoch: 342   Train: 3.9926798343658447   Val: 3.8504626750946045  \n",
      "Epoch: 343   Train: 4.345883846282959    Val: 4.7846360206604     \n",
      "Epoch: 344   Train: 4.150364875793457    Val: 4.167785167694092   \n",
      "Epoch: 345   Train: 3.8775689601898193   Val: 3.8437247276306152  \n",
      "Epoch: 346   Train: 4.071155071258545    Val: 5.3226704597473145  \n",
      "Epoch: 347   Train: 4.206300735473633    Val: 5.579755783081055   \n",
      "Epoch: 348   Train: 4.235264778137207    Val: 4.0896992683410645  \n",
      "Epoch: 349   Train: 4.083702564239502    Val: 4.308703422546387   \n",
      "Epoch: 350   Train: 4.0660858154296875   Val: 4.192843437194824   \n",
      "Epoch: 351   Train: 4.262916088104248    Val: 4.06082820892334    \n",
      "Epoch: 352   Train: 4.3124098777771      Val: 4.372559547424316   \n",
      "Epoch: 353   Train: 3.9497244358062744   Val: 3.7920327186584473  \n",
      "Epoch: 354   Train: 4.043510913848877    Val: 3.9352121353149414  \n",
      "Epoch: 355   Train: 3.8380184173583984   Val: 3.9129374027252197  \n",
      "Epoch: 356   Train: 3.9233832359313965   Val: 4.6584086418151855  \n",
      "Epoch: 357   Train: 4.269070625305176    Val: 9.055523872375488   \n",
      "Epoch: 358   Train: 4.288650989532471    Val: 4.520920753479004   \n",
      "Epoch: 359   Train: 4.111286163330078    Val: 4.437741279602051   \n",
      "Epoch: 360   Train: 3.9129161834716797   Val: 4.762578964233398   \n",
      "Epoch: 361   Train: 4.069492340087891    Val: 4.671351909637451   \n",
      "Epoch: 362   Train: 3.912954330444336    Val: 4.38948392868042    \n",
      "Epoch: 363   Train: 4.039922714233398    Val: 5.3182454109191895  \n",
      "Epoch: 364   Train: 4.203351974487305    Val: 3.9051709175109863  \n",
      "Epoch: 365   Train: 4.238099098205566    Val: 4.737850189208984   \n",
      "Epoch: 366   Train: 4.132674694061279    Val: 4.829202651977539   \n",
      "Epoch: 367   Train: 4.210944652557373    Val: 4.635087013244629   \n",
      "Epoch: 368   Train: 3.886841058731079    Val: 3.8481595516204834  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 369   Train: 4.023162841796875    Val: 5.487575054168701   \n",
      "Epoch: 370   Train: 4.143050670623779    Val: 4.299886703491211   \n",
      "Epoch: 371   Train: 4.0352325439453125   Val: 4.092097759246826   \n",
      "Epoch: 372   Train: 4.055702209472656    Val: 3.819758892059326   \n",
      "Epoch: 373   Train: 3.9091553688049316   Val: 4.917862892150879   \n",
      "Epoch: 374   Train: 4.265767574310303    Val: 6.550236225128174   \n",
      "Epoch: 375   Train: 4.042303085327148    Val: 4.115718841552734   \n",
      "Epoch: 376   Train: 4.070257186889648    Val: 4.5451836585998535  \n",
      "Epoch: 377   Train: 3.929753303527832    Val: 3.8309597969055176  \n",
      "Epoch: 378   Train: 4.01661491394043     Val: 5.085158348083496   \n",
      "Epoch   379: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 379   Train: 4.186267852783203    Val: 4.0551910400390625  \n",
      "Epoch: 380   Train: 3.4834485054016113   Val: 4.308248996734619   \n",
      "Epoch: 381   Train: 3.477229356765747    Val: 3.755924701690674   \n",
      "Epoch: 382   Train: 3.4715535640716553   Val: 3.757484197616577   \n",
      "Epoch: 383   Train: 3.4468491077423096   Val: 3.7302122116088867  \n",
      "Epoch: 384   Train: 3.4893405437469482   Val: 3.826399803161621   \n",
      "Epoch: 385   Train: 3.441925048828125    Val: 3.7876312732696533  \n",
      "Epoch: 386   Train: 3.4569339752197266   Val: 3.741373062133789   \n",
      "Epoch: 387   Train: 3.464421272277832    Val: 3.7408783435821533  \n",
      "Epoch: 388   Train: 3.4681918621063232   Val: 3.7971246242523193  \n",
      "Epoch: 389   Train: 3.478336811065674    Val: 3.743105411529541   \n",
      "Epoch: 390   Train: 3.451004981994629    Val: 3.752586603164673   \n",
      "Epoch: 391   Train: 3.479661464691162    Val: 3.828991174697876   \n",
      "Epoch: 392   Train: 3.4393417835235596   Val: 3.7838730812072754  \n",
      "Epoch: 393   Train: 3.4918036460876465   Val: 3.7612478733062744  \n",
      "Epoch: 394   Train: 3.4768710136413574   Val: 3.873032808303833   \n",
      "Epoch: 395   Train: 3.438570261001587    Val: 3.981975793838501   \n",
      "Epoch: 396   Train: 3.4965317249298096   Val: 3.8439040184020996  \n",
      "Epoch: 397   Train: 3.5080771446228027   Val: 4.001511096954346   \n",
      "Epoch: 398   Train: 3.441967487335205    Val: 3.774282217025757   \n",
      "Epoch: 399   Train: 3.5049047470092773   Val: 3.895242929458618   \n",
      "Epoch: 400   Train: 3.482625961303711    Val: 3.7324581146240234  \n",
      "Epoch: 401   Train: 3.4563732147216797   Val: 3.767725944519043   \n",
      "Epoch: 402   Train: 3.4688680171966553   Val: 3.734328508377075   \n",
      "Epoch: 403   Train: 3.435878276824951    Val: 3.917686700820923   \n",
      "Epoch: 404   Train: 3.5221757888793945   Val: 3.740699529647827   \n",
      "Epoch: 405   Train: 3.456810235977173    Val: 3.751511573791504   \n",
      "Epoch: 406   Train: 3.4543423652648926   Val: 3.73544979095459    \n",
      "Epoch: 407   Train: 3.4767305850982666   Val: 3.7473387718200684  \n",
      "Epoch: 408   Train: 3.4690442085266113   Val: 3.8169431686401367  \n",
      "Epoch   409: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 409   Train: 3.4621360301971436   Val: 3.8032805919647217  \n",
      "Epoch: 410   Train: 3.387460947036743    Val: 3.756009578704834   \n",
      "Epoch: 411   Train: 3.3861334323883057   Val: 3.732172966003418   \n",
      "Epoch: 412   Train: 3.391075372695923    Val: 3.7438278198242188  \n",
      "Epoch: 413   Train: 3.3856756687164307   Val: 3.734696865081787   \n",
      "Epoch: 414   Train: 3.3872666358947754   Val: 3.7446460723876953  \n",
      "Epoch: 415   Train: 3.3874258995056152   Val: 3.7307968139648438  \n",
      "Epoch: 416   Train: 3.387444496154785    Val: 3.732387065887451   \n",
      "Epoch: 417   Train: 3.385608196258545    Val: 3.727756977081299   \n",
      "Epoch: 418   Train: 3.3891353607177734   Val: 3.7470717430114746  \n",
      "Epoch: 419   Train: 3.382157802581787    Val: 3.7277512550354004  \n",
      "Epoch: 420   Train: 3.3887107372283936   Val: 3.7293860912323     \n",
      "Epoch: 421   Train: 3.3890380859375      Val: 3.753385305404663   \n",
      "Epoch: 422   Train: 3.3828628063201904   Val: 3.7712738513946533  \n",
      "Epoch: 423   Train: 3.3924448490142822   Val: 3.7540595531463623  \n",
      "Epoch: 424   Train: 3.388917922973633    Val: 3.723849296569824   \n",
      "Epoch: 425   Train: 3.3853955268859863   Val: 3.727980375289917   \n",
      "Epoch: 426   Train: 3.384875535964966    Val: 3.7316763401031494  \n",
      "Epoch: 427   Train: 3.3828060626983643   Val: 3.7596793174743652  \n",
      "Epoch: 428   Train: 3.3893134593963623   Val: 3.7395598888397217  \n",
      "Epoch: 429   Train: 3.3908004760742188   Val: 3.729198932647705   \n",
      "Epoch: 430   Train: 3.3846991062164307   Val: 3.7299959659576416  \n",
      "Epoch: 431   Train: 3.3845560550689697   Val: 3.7352826595306396  \n",
      "Epoch: 432   Train: 3.3779897689819336   Val: 3.7249765396118164  \n",
      "Epoch: 433   Train: 3.38264536857605     Val: 3.738604784011841   \n",
      "Epoch: 434   Train: 3.386821985244751    Val: 3.732379674911499   \n",
      "Epoch: 435   Train: 3.3812568187713623   Val: 3.7337937355041504  \n",
      "Epoch: 436   Train: 3.378628730773926    Val: 3.7520999908447266  \n",
      "Epoch: 437   Train: 3.384326219558716    Val: 3.7442150115966797  \n",
      "Epoch: 438   Train: 3.383807420730591    Val: 3.746732473373413   \n",
      "Epoch: 439   Train: 3.390561103820801    Val: 3.7287588119506836  \n",
      "Epoch: 440   Train: 3.387444257736206    Val: 3.7262940406799316  \n",
      "Epoch: 441   Train: 3.3839244842529297   Val: 3.724754571914673   \n",
      "Epoch: 442   Train: 3.3824362754821777   Val: 3.725564956665039   \n",
      "Epoch: 443   Train: 3.392575263977051    Val: 3.7255890369415283  \n",
      "Epoch: 444   Train: 3.3860695362091064   Val: 3.728318691253662   \n",
      "Epoch: 445   Train: 3.3872103691101074   Val: 3.7363803386688232  \n",
      "Epoch: 446   Train: 3.3841726779937744   Val: 3.7229387760162354  \n",
      "Epoch: 447   Train: 3.3819212913513184   Val: 3.7385056018829346  \n",
      "Epoch: 448   Train: 3.3899810314178467   Val: 3.7346580028533936  \n",
      "Epoch: 449   Train: 3.3816916942596436   Val: 3.734527826309204   \n",
      "Epoch: 450   Train: 3.384601354598999    Val: 3.7442803382873535  \n",
      "Epoch: 451   Train: 3.3895397186279297   Val: 3.8963210582733154  \n",
      "Epoch: 452   Train: 3.3861680030822754   Val: 3.7300596237182617  \n",
      "Epoch: 453   Train: 3.3928098678588867   Val: 3.7259957790374756  \n",
      "Epoch: 454   Train: 3.382709264755249    Val: 3.724691867828369   \n",
      "Epoch: 455   Train: 3.384525775909424    Val: 3.771745204925537   \n",
      "Epoch: 456   Train: 3.3800132274627686   Val: 3.7320196628570557  \n",
      "Epoch: 457   Train: 3.381939649581909    Val: 3.7406532764434814  \n",
      "Epoch: 458   Train: 3.380377769470215    Val: 3.7280821800231934  \n",
      "Epoch: 459   Train: 3.3888120651245117   Val: 3.733058452606201   \n",
      "Epoch: 460   Train: 3.386939764022827    Val: 3.7641913890838623  \n",
      "Epoch: 461   Train: 3.389556884765625    Val: 3.7330658435821533  \n",
      "Epoch: 462   Train: 3.393380641937256    Val: 3.7292654514312744  \n",
      "Epoch: 463   Train: 3.384423017501831    Val: 3.7340171337127686  \n",
      "Epoch: 464   Train: 3.379345417022705    Val: 3.737600326538086   \n",
      "Epoch: 465   Train: 3.3875396251678467   Val: 3.733715295791626   \n",
      "Epoch: 466   Train: 3.384678363800049    Val: 3.725248336791992   \n",
      "Epoch: 467   Train: 3.385068893432617    Val: 3.729020595550537   \n",
      "Epoch: 468   Train: 3.386715888977051    Val: 3.7270476818084717  \n",
      "Epoch: 469   Train: 3.378640651702881    Val: 3.734733819961548   \n",
      "Epoch: 470   Train: 3.387838125228882    Val: 3.7275218963623047  \n",
      "Epoch: 471   Train: 3.397085189819336    Val: 3.7358880043029785  \n",
      "Epoch   472: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch: 472   Train: 3.385903835296631    Val: 3.7617828845977783  \n",
      "Epoch: 473   Train: 3.3717880249023438   Val: 3.7251803874969482  \n",
      "Epoch: 474   Train: 3.371049404144287    Val: 3.734330415725708   \n",
      "Epoch: 475   Train: 3.3773443698883057   Val: 3.7338690757751465  \n",
      "Epoch: 476   Train: 3.371555805206299    Val: 3.7304129600524902  \n",
      "Epoch: 477   Train: 3.3736002445220947   Val: 3.7313833236694336  \n",
      "Epoch: 478   Train: 3.3734302520751953   Val: 3.738311529159546   \n",
      "Epoch: 479   Train: 3.3706862926483154   Val: 3.7264482975006104  \n",
      "Epoch: 480   Train: 3.3724918365478516   Val: 3.7307753562927246  \n",
      "Epoch: 481   Train: 3.3670358657836914   Val: 3.7305901050567627  \n",
      "Epoch: 482   Train: 3.3705947399139404   Val: 3.726768732070923   \n",
      "Epoch: 483   Train: 3.3732190132141113   Val: 3.724973440170288   \n",
      "Epoch: 484   Train: 3.3682732582092285   Val: 3.73199462890625    \n",
      "Epoch: 485   Train: 3.369478225708008    Val: 3.7249906063079834  \n",
      "Epoch: 486   Train: 3.37233304977417     Val: 3.7330944538116455  \n",
      "Epoch: 487   Train: 3.3731894493103027   Val: 3.7316887378692627  \n",
      "Epoch: 488   Train: 3.3744733333587646   Val: 3.7273755073547363  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 489   Train: 3.373183488845825    Val: 3.7305495738983154  \n",
      "Epoch: 490   Train: 3.3711180686950684   Val: 3.747110605239868   \n",
      "Epoch: 491   Train: 3.3767194747924805   Val: 3.7233726978302     \n",
      "Epoch: 492   Train: 3.3702096939086914   Val: 3.732041835784912   \n",
      "Epoch: 493   Train: 3.372589111328125    Val: 3.740840435028076   \n",
      "Epoch: 494   Train: 3.371333599090576    Val: 3.729280948638916   \n",
      "Epoch: 495   Train: 3.372044086456299    Val: 3.7245073318481445  \n",
      "Epoch: 496   Train: 3.36892032623291     Val: 3.7331926822662354  \n",
      "Epoch: 497   Train: 3.375018358230591    Val: 3.7234134674072266  \n",
      "Epoch   498: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch: 498   Train: 3.369493007659912    Val: 3.7300522327423096  \n",
      "Epoch: 499   Train: 3.373936653137207    Val: 3.728707790374756   \n",
      "Epoch: 500   Train: 3.3711416721343994   Val: 3.7249109745025635  \n",
      "Epoch: 501   Train: 3.3715503215789795   Val: 3.730403184890747   \n",
      "Epoch: 502   Train: 3.3716886043548584   Val: 3.7267305850982666  \n",
      "Epoch: 503   Train: 3.3684496879577637   Val: 3.7304534912109375  \n",
      "Epoch: 504   Train: 3.369292974472046    Val: 3.7258026599884033  \n",
      "Epoch: 505   Train: 3.3684093952178955   Val: 3.727947950363159   \n",
      "Epoch: 506   Train: 3.3673110008239746   Val: 3.727085828781128   \n",
      "Epoch: 507   Train: 3.371473789215088    Val: 3.7263739109039307  \n",
      "Epoch: 508   Train: 3.3690130710601807   Val: 3.7254445552825928  \n",
      "Epoch: 509   Train: 3.373844623565674    Val: 3.7273895740509033  \n",
      "Epoch: 510   Train: 3.369063138961792    Val: 3.7309255599975586  \n",
      "Epoch: 511   Train: 3.370434522628784    Val: 3.728940725326538   \n",
      "Epoch: 512   Train: 3.3700366020202637   Val: 3.731538772583008   \n",
      "Epoch: 513   Train: 3.366581678390503    Val: 3.727729558944702   \n",
      "Epoch: 514   Train: 3.3684897422790527   Val: 3.7294325828552246  \n",
      "Epoch: 515   Train: 3.370516300201416    Val: 3.7293689250946045  \n",
      "Epoch: 516   Train: 3.3663899898529053   Val: 3.724264621734619   \n",
      "Epoch: 517   Train: 3.3710174560546875   Val: 3.730677604675293   \n",
      "Epoch: 518   Train: 3.3721272945404053   Val: 3.727306842803955   \n",
      "Epoch: 519   Train: 3.369218349456787    Val: 3.7276830673217773  \n",
      "Epoch: 520   Train: 3.369760751724243    Val: 3.7288098335266113  \n",
      "Epoch: 521   Train: 3.370058059692383    Val: 3.7254369258880615  \n",
      "Epoch: 522   Train: 3.373295783996582    Val: 3.730588436126709   \n",
      "Epoch: 523   Train: 3.368781328201294    Val: 3.728029489517212   \n",
      "Epoch   524: reducing learning rate of group 0 to 1.0000e-09.\n",
      "Epoch: 524   Train: 3.369480609893799    Val: 3.730811357498169   \n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(DART_model, optimizer, scheduler, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on unseen data (test-set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE =  3.749802350997925 Test RMSE =  4.892120838165283\n"
     ]
    }
   ],
   "source": [
    "results, test_mae, test_rmse = test(DART_model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0, len(epochal_train_losses), 1), epochal_train_losses, label='Training Loss')\n",
    "plt.plot(np.arange(0, len(epochal_train_losses), 1), epochal_val_losses, label='validation loss')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = results[results[:,0].argsort()].cpu().numpy()\n",
    "plt.hist(abs(results[:,1]-results[:,2]), density=True)\n",
    "plt.xlabel(\"MAE\")\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: objects cannot be broadcast to a single shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-47f5edfe32f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msome_res\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cluster size vs MAE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m41\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cluster size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MAE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qmml/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mbar\u001b[0;34m(x, height, width, bottom, align, data, **kwargs)\u001b[0m\n\u001b[1;32m   2487\u001b[0m     return gca().bar(\n\u001b[1;32m   2488\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qmml/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1447\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qmml/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mbar\u001b[0;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[1;32m   2430\u001b[0m         x, height, width, y, linewidth = np.broadcast_arrays(\n\u001b[1;32m   2431\u001b[0m             \u001b[0;31m# Make args iterable too.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2432\u001b[0;31m             np.atleast_1d(x), height, width, y, linewidth)\n\u001b[0m\u001b[1;32m   2433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2434\u001b[0m         \u001b[0;31m# Now that units have been converted, set the tick locations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mbroadcast_arrays\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qmml/lib/python3.7/site-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36mbroadcast_arrays\u001b[0;34m(subok, *args)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubok\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_broadcast_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qmml/lib/python3.7/site-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36m_broadcast_shape\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# use the old-iterator because np.nditer does not handle size 0 arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;31m# consistently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0;31m# unfortunately, it cannot handle 32 or more arguments directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: objects cannot be broadcast to a single shape"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "some_res = np.split(results[:,1:], np.unique(results[:, 0], return_index=True)[1][1:])\n",
    "diff = [abs(i[:,0]-i[:,1]).mean() for i in some_res]\n",
    "plt.title(\"Cluster size vs MAE\")\n",
    "plt.bar(np.arange(31, 41), diff, label='Training Loss')\n",
    "plt.xlabel(\"Cluster size\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = desc_data[train_indices][\"atm_i\"]\n",
    "lol = trainset[:,0].sum(axis=1)\n",
    "sizzle = [i.item() for i in lol]\n",
    "plt.title(\"Train set Cluster distribution, size = {}\".format(len(sizzle)))\n",
    "plt.xlabel(\"Cluster size\")\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.hist(sizzle, bins=10)\n",
    "plt.show()\n",
    "# plt.savefig(\"cluster_distribution_trainset.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = desc_data[test_indices][\"atm_i\"]\n",
    "lol = trainset[:,0].sum(axis=1)\n",
    "sizzle = [i.item() for i in lol]\n",
    "plt.title(\"Test set Cluster distribution, size = {}\".format(len(sizzle)))\n",
    "plt.xlabel(\"Cluster size\")\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.hist(sizzle, bins=10)\n",
    "plt.show()\n",
    "# plt.savefig(\"cluster_distribution_testset.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = desc_data[val_indices][\"atm_i\"]\n",
    "lol = trainset[:,0].sum(axis=1)\n",
    "sizzle = [i.item() for i in lol]\n",
    "plt.title(\"Validation set Cluster distribution, size = {}\".format(len(sizzle)))\n",
    "plt.hist(sizzle, bins=10)\n",
    "plt.xlabel(\"Cluster size\")\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.show()\n",
    "# plt.savefig(\"cluster_distribution_validationset.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:qmml]",
   "language": "python",
   "name": "conda-env-qmml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
